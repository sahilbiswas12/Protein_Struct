{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "from io import StringIO, BytesIO\n",
        "from pathlib import Path\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import os"
      ],
      "metadata": {
        "id": "6Y3Fpq9vemvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UniProtAPIHandler:\n",
        "    \"\"\"\n",
        "    Comprehensive handler for UniProt REST API with support for different data formats\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rate_limit=2):\n",
        "        self.base_url = \"https://rest.uniprot.org\"\n",
        "        self.session = requests.Session()\n",
        "        self.rate_limit = rate_limit\n",
        "\n",
        "        # Proteome IDs for your target species\n",
        "        self.proteome_ids = {\n",
        "            \"human\": \"UP000005640\",\n",
        "            \"fruit_fly\": \"UP000000803\",\n",
        "            \"e_coli\": \"UP000000625\",\n",
        "            \"mouse\": \"UP000000589\",\n",
        "            \"yeast\": \"UP000002311\"\n",
        "        }\n",
        "\n",
        "    def get_proteome_fasta(self, proteome_id, compressed=True, chunk_size=8192):\n",
        "        \"\"\"\n",
        "        Download FASTA sequences for entire proteome\n",
        "        \"\"\"\n",
        "        # Construct API URL\n",
        "        if compressed:\n",
        "            api_url = f\"{self.base_url}/uniprotkb/stream?compressed=true&format=fasta&query=(proteome:{proteome_id})\"\n",
        "        else:\n",
        "            api_url = f\"{self.base_url}/uniprotkb/stream?format=fasta&query=(proteome:{proteome_id})\"\n",
        "\n",
        "        try:\n",
        "            print(f\"Downloading proteome {proteome_id}...\")\n",
        "            response = requests.get(api_url, stream=True, timeout=300)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Handle compressed data\n",
        "            if compressed:\n",
        "                # Get compressed content\n",
        "                compressed_data = b''\n",
        "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                    compressed_data += chunk\n",
        "\n",
        "                # Decompress\n",
        "                with gzip.GzipFile(fileobj=BytesIO(compressed_data)) as gz_file:\n",
        "                    fasta_content = gz_file.read().decode('utf-8')\n",
        "            else:\n",
        "                fasta_content = response.text\n",
        "\n",
        "            print(f\"Downloaded {len(fasta_content):,} characters of FASTA data\")\n",
        "            return fasta_content\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading proteome {proteome_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def parse_fasta_content(self, fasta_content):\n",
        "        \"\"\"\n",
        "        Parse FASTA content into structured data\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "        current_header = None\n",
        "        current_sequence = []\n",
        "\n",
        "        for line in fasta_content.split('\\n'):\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith('>'):\n",
        "                # Save previous sequence if exists\n",
        "                if current_header and current_sequence:\n",
        "                    sequences.append({\n",
        "                        'header': current_header,\n",
        "                        'sequence': ''.join(current_sequence),\n",
        "                        'length': len(''.join(current_sequence))\n",
        "                    })\n",
        "\n",
        "                # Start new sequence\n",
        "                current_header = line[1:]  # Remove '>'\n",
        "                current_sequence = []\n",
        "\n",
        "            elif line and not line.startswith('>'):\n",
        "                current_sequence.append(line)\n",
        "\n",
        "        # Don't forget the last sequence\n",
        "        if current_header and current_sequence:\n",
        "            sequences.append({\n",
        "                'header': current_header,\n",
        "                'sequence': ''.join(current_sequence),\n",
        "                'length': len(''.join(current_sequence))\n",
        "            })\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def extract_uniprot_info(self, fasta_header):\n",
        "        \"\"\"\n",
        "        Extract structured information from FASTA header\n",
        "        \"\"\"\n",
        "        info = {\n",
        "            'uniprot_id': '',\n",
        "            'entry_name': '',\n",
        "            'protein_name': '',\n",
        "            'organism': '',\n",
        "            'organism_id': '',\n",
        "            'gene_name': '',\n",
        "            'protein_existence': '',\n",
        "            'sequence_version': ''\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Split header into parts\n",
        "            parts = fasta_header.split()\n",
        "\n",
        "            # Extract UniProt ID and entry name\n",
        "            if len(parts) > 0:\n",
        "                id_part = parts[0]  # \"sp|P04637|P53_HUMAN\"\n",
        "                if '|' in id_part:\n",
        "                    components = id_part.split('|')\n",
        "                    if len(components) >= 3:\n",
        "                        info['uniprot_id'] = components[1]\n",
        "                        info['entry_name'] = components[2]\n",
        "\n",
        "            # Extract other information\n",
        "            header_str = ' '.join(parts[1:]) if len(parts) > 1 else ''\n",
        "\n",
        "            # Organism (OS=)\n",
        "            if 'OS=' in header_str:\n",
        "                start = header_str.find('OS=') + 3\n",
        "                end = header_str.find(' OX=', start) if ' OX=' in header_str[start:] else len(header_str)\n",
        "                info['organism'] = header_str[start:end].strip()\n",
        "\n",
        "            # Organism ID (OX=)\n",
        "            if 'OX=' in header_str:\n",
        "                start = header_str.find('OX=') + 3\n",
        "                end = header_str.find(' ', start) if ' ' in header_str[start:] else len(header_str)\n",
        "                info['organism_id'] = header_str[start:end].strip()\n",
        "\n",
        "            # Gene name (GN=)\n",
        "            if 'GN=' in header_str:\n",
        "                start = header_str.find('GN=') + 3\n",
        "                end = header_str.find(' ', start) if ' ' in header_str[start:] else len(header_str)\n",
        "                info['gene_name'] = header_str[start:end].strip()\n",
        "\n",
        "            # Protein existence (PE=)\n",
        "            if 'PE=' in header_str:\n",
        "                start = header_str.find('PE=') + 3\n",
        "                end = header_str.find(' ', start) if ' ' in header_str[start:] else len(header_str)\n",
        "                info['protein_existence'] = header_str[start:end].strip()\n",
        "\n",
        "            # Sequence version (SV=)\n",
        "            if 'SV=' in header_str:\n",
        "                start = header_str.find('SV=') + 3\n",
        "                end = header_str.find(' ', start) if ' ' in header_str[start:] else len(header_str)\n",
        "                info['sequence_version'] = header_str[start:end].strip()\n",
        "\n",
        "            # Protein name (everything before OS=)\n",
        "            if 'OS=' in header_str:\n",
        "                protein_name_end = header_str.find(' OS=')\n",
        "                info['protein_name'] = header_str[:protein_name_end].strip()\n",
        "            else:\n",
        "                info['protein_name'] = header_str.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error parsing header: {e}\")\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_proteome_data(self, species_name, max_sequences=None, save_to_file=True):\n",
        "        \"\"\"\n",
        "        Get complete proteome data with parsed information\n",
        "        \"\"\"\n",
        "        if species_name not in self.proteome_ids:\n",
        "            raise ValueError(f\"Unknown species: {species_name}. Available: {list(self.proteome_ids.keys())}\")\n",
        "\n",
        "        proteome_id = self.proteome_ids[species_name]\n",
        "\n",
        "        # Download FASTA data\n",
        "        fasta_content = self.get_proteome_fasta(proteome_id, compressed=True)\n",
        "\n",
        "        if not fasta_content:\n",
        "            return None\n",
        "\n",
        "        # Parse FASTA content\n",
        "        print(\"Parsing FASTA sequences...\")\n",
        "        sequences = self.parse_fasta_content(fasta_content)\n",
        "\n",
        "        # Extract detailed information\n",
        "        print(\"Extracting protein information...\")\n",
        "        proteome_data = []\n",
        "\n",
        "        sequences_to_process = sequences[:max_sequences] if max_sequences else sequences\n",
        "\n",
        "        for i, seq_data in enumerate(tqdm(sequences_to_process, desc=\"Processing sequences\")):\n",
        "            # Extract information from header\n",
        "            protein_info = self.extract_uniprot_info(seq_data['header'])\n",
        "\n",
        "            # Filter out very short sequences\n",
        "            if seq_data['length'] < 20:\n",
        "                continue\n",
        "\n",
        "            # Combine all information\n",
        "            protein_entry = {\n",
        "                'species': species_name,\n",
        "                'proteome_id': proteome_id,\n",
        "                'uniprot_id': protein_info['uniprot_id'],\n",
        "                'entry_name': protein_info['entry_name'],\n",
        "                'protein_name': protein_info['protein_name'],\n",
        "                'gene_name': protein_info['gene_name'],\n",
        "                'organism': protein_info['organism'],\n",
        "                'organism_id': protein_info['organism_id'],\n",
        "                'protein_existence': protein_info['protein_existence'],\n",
        "                'sequence_version': protein_info['sequence_version'],\n",
        "                'sequence': seq_data['sequence'],\n",
        "                'sequence_length': seq_data['length'],\n",
        "                'full_header': seq_data['header']\n",
        "            }\n",
        "\n",
        "            proteome_data.append(protein_entry)\n",
        "\n",
        "        print(f\"Processed {len(proteome_data)} protein sequences for {species_name}\")\n",
        "\n",
        "        # Save to file if requested\n",
        "        if save_to_file:\n",
        "            output_file = f\"{species_name}_proteome_data.json\"\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(proteome_data, f, indent=2)\n",
        "            print(f\"Data saved to {output_file}\")\n",
        "\n",
        "        return proteome_data"
      ],
      "metadata": {
        "id": "G1zDuINsfSTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing UniProt API Handler...\")\n",
        "handler = UniProtAPIHandler(rate_limit=2)\n",
        "print(f\"Handler initialized for species: {list(handler.proteome_ids.keys())}\")\n",
        "\n",
        "# Step 4: Test with E. coli sample\n",
        "print(\"\\nTesting with E. coli sample...\")\n",
        "try:\n",
        "    ecoli_sample = handler.get_proteome_data(\"e_coli\", max_sequences=50)\n",
        "\n",
        "    if ecoli_sample:\n",
        "        print(f\"Successfully downloaded {len(ecoli_sample)} E. coli proteins\")\n",
        "\n",
        "        # Convert to DataFrame for easy analysis\n",
        "        df = pd.DataFrame(ecoli_sample)\n",
        "        print(\"\\nQuick analysis:\")\n",
        "        print(f\"Average sequence length: {df['sequence_length'].mean():.1f}\")\n",
        "        print(f\"Sequence length range: {df['sequence_length'].min()} - {df['sequence_length'].max()}\")\n",
        "        print(f\"Unique gene names: {df['gene_name'].nunique()}\")\n",
        "\n",
        "        # Show sample protein\n",
        "        print(f\"\\n Sample protein:\")\n",
        "        sample = ecoli_sample[0]\n",
        "        print(f\"  UniProt ID: {sample['uniprot_id']}\")\n",
        "        print(f\"  Gene name: {sample['gene_name']}\")\n",
        "        print(f\"  Protein name: {sample['protein_name'][:60]}...\")\n",
        "        print(f\"  Sequence length: {sample['sequence_length']}\")\n",
        "        print(f\"  Sequence preview: {sample['sequence'][:60]}...\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to download E. coli data\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\nReady to proceed with full data collection!\")\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Run handler.get_proteome_data('species_name', max_sequences=N) for each species\")\n",
        "print(\"2. Adjust max_sequences based on your needs\")\n",
        "print(\"3. Combine all species data for your ML pipeline\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SMG7qDPflFi",
        "outputId": "67a2683d-11b4-4f90-98a5-ed36e6e5a1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing UniProt API Handler...\n",
            "Handler initialized for species: ['human', 'fruit_fly', 'e_coli', 'mouse', 'yeast']\n",
            "\n",
            "Testing with E. coli sample...\n",
            "Downloading proteome UP000000625...\n",
            "Downloaded 1,890,840 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 50/50 [00:00<00:00, 57582.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50 protein sequences for e_coli\n",
            "Data saved to e_coli_proteome_data.json\n",
            "Successfully downloaded 50 E. coli proteins\n",
            "\n",
            "Quick analysis:\n",
            "Average sequence length: 516.4\n",
            "Sequence length range: 31 - 1073\n",
            "Unique gene names: 50\n",
            "\n",
            " Sample protein:\n",
            "  UniProt ID: A5A616\n",
            "  Gene name: mgtS\n",
            "  Protein name: Small protein MgtS...\n",
            "  Sequence length: 31\n",
            "  Sequence preview: MLGNMNVFMAVLGIILFSGFLAAYFSHKWDD...\n",
            "\n",
            "Ready to proceed with full data collection!\n",
            "Next steps:\n",
            "1. Run handler.get_proteome_data('species_name', max_sequences=N) for each species\n",
            "2. Adjust max_sequences based on your needs\n",
            "3. Combine all species data for your ML pipeline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_all_species_data(handler, species_targets=None):\n",
        "    \"\"\"\n",
        "    Collect protein data for all target species\n",
        "    \"\"\"\n",
        "\n",
        "    # Default targets (adjust based on your computational resources)\n",
        "    if species_targets is None:\n",
        "        species_targets = {\n",
        "            \"e_coli\": 400,      # Smallest - good for testing\n",
        "            \"yeast\": 600,       # Small-medium\n",
        "            \"fruit_fly\": 800,   # Medium\n",
        "            \"mouse\": 1000,      # Large\n",
        "            \"human\": 1200       # Largest\n",
        "        }\n",
        "\n",
        "    print(\"Starting data collection for all species...\")\n",
        "    print(f\"Targets: {species_targets}\")\n",
        "\n",
        "    all_protein_data = {}\n",
        "    collection_summary = {}\n",
        "\n",
        "    for species, max_count in species_targets.items():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PROCESSING: {species.upper()}\")\n",
        "        print(f\"Target: {max_count} proteins\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Download data\n",
        "            protein_data = handler.get_proteome_data(\n",
        "                species,\n",
        "                max_sequences=max_count,\n",
        "                save_to_file=True\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            if protein_data:\n",
        "                all_protein_data[species] = protein_data\n",
        "\n",
        "                # Calculate statistics\n",
        "                lengths = [p['sequence_length'] for p in protein_data]\n",
        "                collection_summary[species] = {\n",
        "                    'count': len(protein_data),\n",
        "                    'avg_length': sum(lengths) / len(lengths),\n",
        "                    'min_length': min(lengths),\n",
        "                    'max_length': max(lengths),\n",
        "                    'download_time': end_time - start_time\n",
        "                }\n",
        "\n",
        "                print(f\"{species}: {len(protein_data)} proteins collected\")\n",
        "                print(f\"   Average length: {collection_summary[species]['avg_length']:.1f}\")\n",
        "                print(f\"   Time taken: {collection_summary[species]['download_time']:.1f} seconds\")\n",
        "            else:\n",
        "                print(f\" Failed to collect {species} data\")\n",
        "                collection_summary[species] = {'count': 0, 'error': 'Download failed'}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {species}: {e}\")\n",
        "            collection_summary[species] = {'count': 0, 'error': str(e)}\n",
        "\n",
        "        # Small delay between species\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Print final summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DATA COLLECTION COMPLETE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    total_proteins = 0\n",
        "    for species, summary in collection_summary.items():\n",
        "        count = summary.get('count', 0)\n",
        "        total_proteins += count\n",
        "\n",
        "        if count > 0:\n",
        "            print(f\"{species:<12}: {count:>5,} proteins (avg length: {summary['avg_length']:>6.1f})\")\n",
        "        else:\n",
        "            error = summary.get('error', 'Unknown error')\n",
        "            print(f\"{species:<12}: Failed - {error}\")\n",
        "\n",
        "    print(f\"\\nTOTAL COLLECTED: {total_proteins:,} proteins across {len(all_protein_data)} species\")\n",
        "\n",
        "    return all_protein_data, collection_summary\n",
        "\n",
        "def create_combined_dataset(all_protein_data):\n",
        "    \"\"\"\n",
        "    Create a combined dataset from all species\n",
        "    \"\"\"\n",
        "    print(\"\\n Creating combined dataset...\")\n",
        "\n",
        "    combined_sequences = []\n",
        "    combined_metadata = []\n",
        "\n",
        "    for species, protein_list in all_protein_data.items():\n",
        "        print(f\"  Adding {len(protein_list)} proteins from {species}...\")\n",
        "\n",
        "        for protein in protein_list:\n",
        "            combined_sequences.append(protein['sequence'])\n",
        "            combined_metadata.append({\n",
        "                'species': species,\n",
        "                'uniprot_id': protein['uniprot_id'],\n",
        "                'gene_name': protein['gene_name'],\n",
        "                'protein_name': protein['protein_name'],\n",
        "                'length': protein['sequence_length'],\n",
        "                'organism': protein['organism']\n",
        "            })\n",
        "\n",
        "    # Create final dataset\n",
        "    final_dataset = {\n",
        "        'sequences': combined_sequences,\n",
        "        'metadata': combined_metadata,\n",
        "        'collection_info': {\n",
        "            'total_sequences': len(combined_sequences),\n",
        "            'species_count': len(all_protein_data),\n",
        "            'collection_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save combined dataset\n",
        "    with open('combined_protein_dataset.json', 'w') as f:\n",
        "        json.dump(final_dataset, f, indent=2)\n",
        "\n",
        "    print(f\" Combined dataset created: {len(combined_sequences):,} sequences\")\n",
        "    print(f\" Saved to: combined_protein_dataset.json\")\n",
        "\n",
        "    return final_dataset\n",
        "\n",
        "def analyze_dataset(final_dataset):\n",
        "    \"\"\"\n",
        "    Perform basic analysis of the combined dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n DATASET ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create DataFrame for analysis\n",
        "    df = pd.DataFrame(final_dataset['metadata'])\n",
        "    df['sequence'] = final_dataset['sequences']\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"Total proteins: {len(df):,}\")\n",
        "    print(f\"Average length: {df['length'].mean():.1f}\")\n",
        "    print(f\"Length std: {df['length'].std():.1f}\")\n",
        "    print(f\"Length range: {df['length'].min()} - {df['length'].max()}\")\n",
        "\n",
        "    print(f\"\\n SPECIES DISTRIBUTION:\")\n",
        "    species_counts = df['species'].value_counts()\n",
        "    for species, count in species_counts.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {species:<12}: {count:>5,} ({percentage:>5.1f}%)\")\n",
        "\n",
        "    print(f\"\\n LENGTH STATISTICS BY SPECIES:\")\n",
        "    length_stats = df.groupby('species')['length'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
        "    print(length_stats)\n",
        "\n",
        "    # Amino acid composition analysis\n",
        "    print(f\"\\n AMINO ACID COMPOSITION ANALYSIS:\")\n",
        "    all_sequences = ''.join(df['sequence'])\n",
        "    aa_counts = {}\n",
        "    for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
        "        count = all_sequences.count(aa)\n",
        "        percentage = (count / len(all_sequences)) * 100\n",
        "        aa_counts[aa] = {'count': count, 'percentage': percentage}\n",
        "\n",
        "    # Sort by frequency\n",
        "    sorted_aa = sorted(aa_counts.items(), key=lambda x: x[1]['count'], reverse=True)\n",
        "\n",
        "    print(\"  Top 10 most frequent amino acids:\")\n",
        "    for aa, stats in sorted_aa[:10]:\n",
        "        print(f\"    {aa}: {stats['count']:>8,} ({stats['percentage']:>5.2f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main execution function\n",
        "def run_full_collection():\n",
        "    \"\"\"\n",
        "    Run the complete data collection pipeline\n",
        "    \"\"\"\n",
        "    print(\"STARTING FULL PROTEIN DATA COLLECTION PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Make sure handler is initialized\n",
        "    if 'handler' not in globals():\n",
        "        print(\" Error: UniProtAPIHandler not found. Please run the previous code block first.\")\n",
        "        return None\n",
        "\n",
        "    # Step 1: Collect data for all species\n",
        "    all_data, summary = collect_all_species_data(handler)\n",
        "\n",
        "    if not all_data:\n",
        "        print(\" No data collected. Aborting.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Create combined dataset\n",
        "    combined_dataset = create_combined_dataset(all_data)\n",
        "\n",
        "    # Step 3: Analyze the dataset\n",
        "    df_analysis = analyze_dataset(combined_dataset)\n",
        "\n",
        "    print(f\"\\n PIPELINE COMPLETE!\")\n",
        "    print(f\" Individual species files saved (JSON format)\")\n",
        "    print(f\" Combined dataset saved: combined_protein_dataset.json\")\n",
        "    print(f\" Ready for deep learning pipeline!\")\n",
        "\n",
        "    return combined_dataset, df_analysis"
      ],
      "metadata": {
        "id": "rC8t3bigfvMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset, analysis_df = run_full_collection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GED5fhIYf7nz",
        "outputId": "f4d91dd2-1298-4409-a434-265470d408c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING FULL PROTEIN DATA COLLECTION PIPELINE\n",
            "============================================================\n",
            "Starting data collection for all species...\n",
            "Targets: {'e_coli': 400, 'yeast': 600, 'fruit_fly': 800, 'mouse': 1000, 'human': 1200}\n",
            "\n",
            "============================================================\n",
            "PROCESSING: E_COLI\n",
            "Target: 400 proteins\n",
            "============================================================\n",
            "Downloading proteome UP000000625...\n",
            "Downloaded 1,890,840 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 400/400 [00:00<00:00, 40274.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 400 protein sequences for e_coli\n",
            "Data saved to e_coli_proteome_data.json\n",
            "e_coli: 400 proteins collected\n",
            "   Average length: 375.9\n",
            "   Time taken: 1.5 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PROCESSING: YEAST\n",
            "Target: 600 proteins\n",
            "============================================================\n",
            "Downloading proteome UP000002311...\n",
            "Downloaded 3,855,264 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 600/600 [00:00<00:00, 34395.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 600 protein sequences for yeast\n",
            "Data saved to yeast_proteome_data.json\n",
            "yeast: 600 proteins collected\n",
            "   Average length: 552.0\n",
            "   Time taken: 2.7 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PROCESSING: FRUIT_FLY\n",
            "Target: 800 proteins\n",
            "============================================================\n",
            "Downloading proteome UP000000803...\n",
            "Downloaded 17,595,317 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 800/800 [00:00<00:00, 51642.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 800 protein sequences for fruit_fly\n",
            "Data saved to fruit_fly_proteome_data.json\n",
            "fruit_fly: 800 proteins collected\n",
            "   Average length: 1733.5\n",
            "   Time taken: 15.4 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PROCESSING: MOUSE\n",
            "Target: 1000 proteins\n",
            "============================================================\n",
            "Downloading proteome UP000000589...\n",
            "Downloaded 29,420,930 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 1000/1000 [00:00<00:00, 122115.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 protein sequences for mouse\n",
            "Data saved to mouse_proteome_data.json\n",
            "mouse: 1000 proteins collected\n",
            "   Average length: 964.8\n",
            "   Time taken: 30.4 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PROCESSING: HUMAN\n",
            "Target: 1200 proteins\n",
            "============================================================\n",
            "Downloading proteome UP000005640...\n",
            "Downloaded 40,649,282 characters of FASTA data\n",
            "Parsing FASTA sequences...\n",
            "Extracting protein information...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sequences: 100%|██████████| 1200/1200 [00:00<00:00, 26482.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1199 protein sequences for human\n",
            "Data saved to human_proteome_data.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human: 1199 proteins collected\n",
            "   Average length: 680.1\n",
            "   Time taken: 30.9 seconds\n",
            "\n",
            "============================================================\n",
            "DATA COLLECTION COMPLETE!\n",
            "============================================================\n",
            "e_coli      :   400 proteins (avg length:  375.9)\n",
            "yeast       :   600 proteins (avg length:  552.0)\n",
            "fruit_fly   :   800 proteins (avg length: 1733.5)\n",
            "mouse       : 1,000 proteins (avg length:  964.8)\n",
            "human       : 1,199 proteins (avg length:  680.1)\n",
            "\n",
            "TOTAL COLLECTED: 3,999 proteins across 5 species\n",
            "\n",
            " Creating combined dataset...\n",
            "  Adding 400 proteins from e_coli...\n",
            "  Adding 600 proteins from yeast...\n",
            "  Adding 800 proteins from fruit_fly...\n",
            "  Adding 1000 proteins from mouse...\n",
            "  Adding 1199 proteins from human...\n",
            " Combined dataset created: 3,999 sequences\n",
            " Saved to: combined_protein_dataset.json\n",
            "\n",
            " DATASET ANALYSIS\n",
            "==================================================\n",
            "Total proteins: 3,999\n",
            "Average length: 912.4\n",
            "Length std: 1385.8\n",
            "Length range: 31 - 35213\n",
            "\n",
            " SPECIES DISTRIBUTION:\n",
            "  human       : 1,199 ( 30.0%)\n",
            "  mouse       : 1,000 ( 25.0%)\n",
            "  fruit_fly   :   800 ( 20.0%)\n",
            "  yeast       :   600 ( 15.0%)\n",
            "  e_coli      :   400 ( 10.0%)\n",
            "\n",
            " LENGTH STATISTICS BY SPECIES:\n",
            "           count    mean     std  min    max\n",
            "species                                     \n",
            "e_coli       400   375.9   244.8   31   1486\n",
            "fruit_fly    800  1733.5  2280.9   63  22949\n",
            "human       1199   680.1   703.1   31   8923\n",
            "mouse       1000   964.8  1414.1   34  35213\n",
            "yeast        600   552.0   422.2   59   2222\n",
            "\n",
            " AMINO ACID COMPOSITION ANALYSIS:\n",
            "  Top 10 most frequent amino acids:\n",
            "    L:  333,762 ( 9.15%)\n",
            "    S:  298,740 ( 8.19%)\n",
            "    E:  264,440 ( 7.25%)\n",
            "    A:  260,093 ( 7.13%)\n",
            "    G:  228,986 ( 6.28%)\n",
            "    V:  223,770 ( 6.13%)\n",
            "    P:  214,410 ( 5.88%)\n",
            "    K:  212,621 ( 5.83%)\n",
            "    T:  208,359 ( 5.71%)\n",
            "    D:  197,187 ( 5.40%)\n",
            "\n",
            " PIPELINE COMPLETE!\n",
            " Individual species files saved (JSON format)\n",
            " Combined dataset saved: combined_protein_dataset.json\n",
            " Ready for deep learning pipeline!\n"
          ]
        }
      ]
    }
  ]
}